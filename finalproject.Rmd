---
title: "Assignment 4"
subtitle: "DATA 622 Final Project"
author: "Stephanie Chiang"
date: "Spring 2025"
output:
  html_document:
    df_print: paged
---


# Introduction

As (laid-off) software engineer, I have personally had concerns with how a career in the current incarnation of the technology industry may relate to individuals' mental health. There are so many issues generating endless discussion in traditional and new media that directly affect tech work and workers, like the perception of the capabilities of AI on job prospects, the effects of increased social media use on society, the hardware economy and geopolitics, etc.

My focus in this analysis is to examine any possible relationships between the reporting of mental health issues in tech workers and the characteristics of the companies that employ them, like the culture of the workplace and the resources provided. This information could be valuable in the development of actionable changes by tech employers to foster more supportive and rewarding environments. I believe it's important to remind and reiterate to companies and their decision-makers that the health and well-being of their employees is the base upon which their businesses are built.


# Exploratory Data Analysis

```{r lib, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(e1071)
library(randomForest)
library(neuralnet)
library(keras)
```

The (data)[https://data.mendeley.com/datasets/mmnzx4w8cg/1] for this study comes from OSMI (Open Sourcing Mental Illness) a non-profit that aims to provide resources and raise awareness of mental health issues in the tech community. This particular dataset is a (compilation)[https://www.sciencedirect.com/science/article/pii/S2352340924003469] of survey responses from workers in tech jobs, collected from 2017 to 2021.

### The Variables

The response variable will be `mental_health`, described as "Whether or not respondents currently have a mental health disorder" as self-reported on the surveys. To me, this variable is less an indication of the actual rates of poor mental health in employees than it is an expression of the rate of openness/awareness of those issues. In other words, the data will technically be trained to predict "Yes" if employees feel safe enough to talk about sensitive personal issues in the context of their workplace, even when the survey is anonymized.

Some of the predictor variables include:
- `tech_company`	Whether or not the respondent's employer is primarily a tech company/organization.
- `benefits`	Whether or not the employer provides mental health benefits as part of healthcare coverage.
- `medical_coverage` Whether or not respondents have medical coverage (private insurance or state-provided) that includes treatment of mental health disorders.
- `workplace_resources`	Whether or not the employer offers resources to learn more about mental health disorders and options for seeking help.
- `mh_employer_discussion`	Whether or not the respondent has ever discussed their mental health with the employer.
- `mh_coworker_discussion` Whether or not the respondent has ever discussed their mental health with coworkers
- `mh_share` The willingness of respondent to share mental health illness issues with friends and family, on a scale from 0 to 10.

```{r import}
osmi_raw <- read.csv(file = "osmi.csv")
glimpse(osmi_raw)
table(osmi$country)
```

The data will be cleaned and transformed as follows:
- some unclear or unsure responses are converted to `NA`s
- character columns are converted to factors with no more than 3 levels
- `country` column will be converted to "USA" or "Other" since the majority of respondents are from the US

Also, as discussed above, since the goal is binary classification made on the `mental_health` column, the "Don't Know" value will be converted to "No" and the "Possibly" value will be interpreted as "Yes" (interpreted here as comfortable enough in their workplace to answer affirmatively).

```{r clean}
osmi <- osmi_raw

osmi <- osmi |>
  mutate(benefits = na_if(benefits, "I don't know"),
         workplace_resources = na_if(workplace_resources, "I don't know"),
         country = case_match(country, "United States of America" ~ "USA", .default = "Other"),
         mental_health = case_match(mental_health, "Don't Know" ~ "No", .default = mental_health),
         mental_health = case_match(mental_health, "Possibly" ~ "Yes", .default = mental_health)) |>
  mutate(across(where(is.character), as.factor))

head(osmi)
```

### Distributions

One measure of respondents' mental health in this dataset is the employees' "Willingness to share" mental health illness issues with friends and family. Here are the distributions by age and gender, with slightly higher frequency for workers in their 30s and with gender identity reported as 'other'.

```{r demo, message=FALSE, warning=FALSE}
ggplot(osmi, aes(x = age, y = mh_share)) +
  geom_jitter() + 
  geom_smooth() +
  labs(y = "Willingness to share", x = "Age")

ggplot(osmi, aes(x = gender, y = mh_share)) +
  geom_boxplot() + 
  labs(y = "Willingness to share", x = "")
```
The binary categorical variables representing company resources and culture are visualized below. There clear majorities in the data for companies that are in the tech industry and for employees having never discussed mental health with the employer, even though more employees reported "Yes" for having mental health issues.

```{r bar}
ggplot(osmi, aes(x = tech_company)) +
  geom_bar() +
  labs(title = "Whether the respondent's employer is primarily a tech company", x = "", y = "")

ggplot(osmi, aes(x = mh_employer_discussion)) +
  geom_bar() +
  labs(title = "Whether respondent has ever discussed their mental health with the employer", x = "", y = "")

ggplot(osmi, aes(x = mh_coworker_discussion)) +
  geom_bar() +
  labs(title = "Whether respondent has ever discussed their mental health with coworkers", x = "", y = "")

ggplot(osmi, aes(x = mental_health)) +
  geom_bar() +
  labs(title = "Whether respondent currently reports a mental health issue", x = "", y = "")
```

# Algorithm Selection

## Support Vector Machines

Since the dataset is relatively small with only 1242 observations, I have chosen to begin with SVMs because these models can be effective at avoiding overfitting during the training stage, which could be a concern for a decision tree or even ensembles of trees like XGBoost. Cross-validation can be applied to help further tune hyperparameters and increase accuracy and efficiency.

## Neural Networks

I have also selected a Multilayer Perceptron (MLP) aka a simple feedforward neural network because the goal is a simple binary classification task. Other types of neural networks tend to work well on larger datasets and complex problems like image processing, which could be overkill on this more focused task.


# Experimentation & Model Training

The data is split for training and testing. The `NA`s are imputed using `na.roughfix` which replaces missing numeric values with medians and factor variables with the most frequent levels (breaking ties at random).

```{r split}
set.seed(101)

splitIndex <- createDataPartition(osmi$mental_health, p = 0.8, list = FALSE)

osmi_train <- osmi[splitIndex,]
osmi_test <- osmi[-splitIndex,]

osmi_train <- na.roughfix(osmi_train)
osmi_test <- na.roughfix(osmi_test)

round(prop.table(table(select(osmi, mental_health))), 2)
round(prop.table(table(select(osmi_train, mental_health))), 2)
round(prop.table(table(select(osmi_test, mental_health))), 2)
```

```{r tracking}
experiment_log <- data.frame(
  ID = integer(),
  Model = character(),
  Features = character(),
  Hyperparameters = character(),
  Train = numeric(),
  Test = numeric(),
  Notes = character(),
  stringsAsFactors = FALSE
)
```

## Support Vector Machines

### Experiment 1:

Objective: Since SVMs generalize well on smaller datasets like this one, this can be a baseline for accuracy comparisons.

Variations: This first model will use a linear kernel with the default cost (hardness/softness of margin) of 1. The missing values must be imputed, here using `na.roughfix`. The model has built-in scaling for the numeric columns to balance the features' contributions in determining the hyperplane.
      
Evaluation: Generating the confusion matrix for accuracy.

Experiment:

```{r svm1-mod}
set.seed(101)

num_cols <- sapply(osmi_train, is.numeric)

svm1 <- svm(mental_health ~.,
            data = osmi_train,
            scale = num_cols,
            kernel = "linear")

summary(svm1)
```

```{r svm1-eval}
# predict and evaluate on training data
svm1_train_pred <- predict(svm1, osmi_train)
svm1_train_cm <- confusionMatrix(svm1_train_pred, osmi_train$mental_health)
svm1_train_cm$overall["Accuracy"]

# predict and evaluate on testing data
svm1_test_pred <- predict(svm1, osmi_test)
svm1_test_cm <- confusionMatrix(svm1_test_pred, osmi_test$mental_health)
svm1_test_cm$overall["Accuracy"]
```
Review: The accuracy for rates for training and testing indicate a decent performance; but improvement would be ideal.

```{r svm1-log}
svm1_log <- data.frame(
  ID = 1,
  Model = "SVM",
  Features = "all",
  Hyperparameters = "cost = 1",
  Train = 0.70,
  Test = 0.66,
  Notes = "mediocre predictive accuracy"
)

experiment_log <- bind_rows(experiment_log, svm1_log)
```


### Experiment 2:

Objective: Since the dataset is on the smaller side, the kernel will be adjusted to see if the accuracy can be increased.

Variations: The kernel will be changed to the nonlinear RBF (Radial Basis Function), which is more flexible and able to read unclear patterns.

Evaluation: Generating the confusion matrix for accuracy.

Experiment:

```{r svm2-mod}
svm2 <- svm(mental_health ~.,
            data = osmi_train,
            scale = num_cols,
            kernel = "radial")

summary(svm2)
```

```{r svm1-eval}
# predict and evaluate on training data
svm2_train_pred <- predict(svm2, osmi_train)
svm2_train_cm <- confusionMatrix(svm2_train_pred, osmi_train$mental_health)
svm2_train_cm$overall["Accuracy"]

# predict and evaluate on testing data
svm2_test_pred <- predict(svm2, osmi_test)
svm2_test_cm <- confusionMatrix(svm2_test_pred, osmi_test$mental_health)
svm2_test_cm$overall["Accuracy"]
```

Review: The accuracy improved slightly, but with a larger difference between training and testing.

```{r svm1-log}
svm2_log <- data.frame(
  ID = 2,
  Model = "SVM",
  Features = "all",
  Hyperparameters = "kernel = radial",
  Train = 0.74,
  Test = 0.68,
  Notes = "slight improvement and overfitting"
)

experiment_log <- bind_rows(experiment_log, svm2_log)
```


### Experiment 3:

Objective: 10-fold cross-validation will be applied with different, commonly-used `cost` and `gamma` values to determine the best-performing hyperparameters for a final SVM test.

Variations: Based on the above, the cost may be changed to 0.01, 0.1, 1 (same), or 10. The best gamma will be chosen from 0.001, 0.025 (default), 0.1, or 1.
      
Evaluation: Generating the confusion matrix for accuracy.

Experiment:

```{r svm-tune}
tune_mod <- tune(svm,
                 mental_health ~.,
                 data = osmi_train,
                 kernel = "radial",
                 ranges = list(cost = c(0.01, 0.1, 1, 10), gamma = c(0.001, 0.025, 0.1, 1)))

summary(tune_mod)
```

```{r tune-eval}
best_mod <- tune_mod$best.model

# predict and evaluate on training data
best_train_pred <- predict(best_mod, osmi_train)
best_train_cm <- confusionMatrix(best_train_pred, osmi_train$mental_health)
best_train_cm$overall["Accuracy"]

# predict and evaluate on testing data
best_test_pred <- predict(best_mod, osmi_test)
best_test_cm <- confusionMatrix(best_test_pred, osmi_test$mental_health)
best_test_cm$overall["Accuracy"]
```

Review: The hyperparameter tuning determined that the best model raised the `gamma` to 0.1, which reduced the effect of points further from the decision boundary; as expected, this resulted in the model overfitting on training data. 

The performance of SVM on this dataset has been passable, but there may be better algorithms to try.

```{r svm3-log}
svm3_log <- data.frame(
  ID = 3,
  Model = "SVM",
  Features = "all",
  Hyperparameters = "tuned to best gamma 0.1",
  Train = 0.72,
  Test = 0.65,
  Notes = "modest increase in overfitting"
)

experiment_log <- bind_rows(experiment_log, svm3_log)
```


## Simple Neural Network

### Experiment 4:

Objective: We will test whether a simple neural network can predict more effectively and accurately on this dataset than SVM.

Variations: 
- In a simple neural network, features must be hand-selected for the formula. A simple `randomForest` can rank importance using `MeanDecreaseGini` aka the reduction in impurity (business logic could be applied instead, but a RF was more reproducible). The top 5 features will be used.
- Factor columns must be converted to dummy variables (one-hot encoded), then converted back for interpretation.
- The neural network will have 2 hidden layers, the first layer with 3 neurons and the second with 2.
      
Evaluation: Generating the confusion matrix for accuracy.

Experiment:

```{r features}
rf <- randomForest(mental_health ~ ., data = osmi_train)
importance(rf)
```

```{r nn1-scale1h}
otrain_scaled <- osmi_train
otrain_scaled[num_cols] <- scale(otrain_scaled[num_cols])
mh_1h_train <- ifelse(otrain_scaled$mental_health == "Yes", 1, 0)
otrain_scaled <- model.matrix(mental_health ~ ., data = otrain_scaled)[, -1]
otrain_scaled <- cbind(mental_health = mh_1h_train, otrain_scaled)

otest_scaled <- osmi_test
otest_scaled[num_cols] <- scale(otest_scaled[num_cols])
mh_1h_test <- ifelse(otest_scaled$mental_health == "Yes", 1, 0)
otest_scaled <- model.matrix(mental_health ~ ., data = otest_scaled)[, -1]
otest_scaled <- cbind(mental_health = mh_1h_test, otest_scaled)
```

```{r nn1-mod}
nn1 <- neuralnet(mental_health ~ age + mh_share + mh_coworker_discussionYes + mh_employer_discussionYes + countryUSA,
                 data = otrain_scaled,
                 hidden = c(3, 2))

summary(nn1)

plot(nn1, rep = "best")
```

```{r nn1-eval}
# predict and evaluate on training data
nn1_train_pred <- predict(nn1, otrain_scaled)

# convert probabilities back to binary yes/no at threshold = 0.5,
nn1_train_pred_factor <- ifelse(nn1_train_pred > 0.5, "Yes", "No")

# then back to factors for the confusion matrix
nn1_train_pred_factor <- factor(nn1_train_pred_factor, levels = c("No", "Yes"))
nn1_train_cm <- confusionMatrix(nn1_train_pred_factor, osmi_train$mental_health, positive = "Yes")
nn1_train_cm$overall["Accuracy"]

# predict and evaluate on testing data
nn1_test_pred <- predict(nn1, otest_scaled)
nn1_test_pred_factor <- ifelse(nn1_test_pred > 0.5, "Yes", "No")
nn1_test_pred_factor <- factor(nn1_test_pred_factor, levels = c("No", "Yes"))
nn1_test_cm <- confusionMatrix(nn1_test_pred_factor, osmi_test$mental_health, positive = "Yes")
nn1_test_cm$overall["Accuracy"]
```

Review: 

```{r nn1-log}
nn1_log <- data.frame(
  ID = 4,
  Model = "Simple Neural Net",
  Features = "Top 5 based on RF",
  Hyperparameters = "",
  Train = 0.72,
  Test = 0.65,
  Notes = "similar results to SVM"
)

experiment_log <- bind_rows(experiment_log, nn1_log)
```


### Experiment 5:

Objective: W

Variations: 
- 
- The neural network will have 2 hidden layers, the first layer with 4 neurons and the second with 2.
      
Evaluation: Generating the confusion matrix for accuracy.

Experiment:







# Results & Comparison 

Make your conclusions from your analysis. Please be sure to address the business impact (it could be of any domain) of your solution.


Deliverable:
Your final presentation (essay or video) should include:
The traditional R file or Python file and essay,
An Essay (minimum 500 word document) or Video (5 to 8 minutes recording)
Include the execution and explanation of your code. 

***

Data Selection 10	
Appropriate data selected

Trained Models 10	
Models trained on chosen algorithms

Problem Description	10	
1. Business problem defined (5)
2. Business problem translated to data science problem (5)

Dataset Analysis	10	
1. Exploratory data analysis (5)
2. Experimentation performed (5)

Methodologies used	10	
Two different algorithms chosen

Conclusions - with business impact	20	
1. Comparison of results (10)
2. Business impact included (10)

Essay/Video	30	
1. Essay was at least 500 words/video was at least 5 minutes (10)
2. Quality of work (10)
3. Summary with business impact (10)
